# Putusan Mahkamah Agung Crawler using Scrapy, Apache Airflow, ClickHouse and S3 MinIO

This is a 1+ month internship project that helps telecom companies enrich their user data by scraping Supreme Court decisions (putusan mahkamah agung) and extracting descriptions from them.

## Tech Stack Overview

This project was both challenging and enjoyable to work on.

- **Scrapy**: Handles web crawling and page extraction
- **ClickHouse**: Stores and enables fast querying of crawled data
- **Apache Airflow**: Orchestrates the crawler
- **S3 MinIO**: Data lake for storing PDFs

## Project Structure

```
putusan-ma-crawler/
â”œâ”€â”€ ğŸ“ airflow/                          # Apache Airflow orchestration
â”‚   â”œâ”€â”€ ğŸ“ config/
â”‚   â”‚   â””â”€â”€ ğŸ“„ airflow.cfg               # Airflow configuration
â”‚   â””â”€â”€ ğŸ“ dags/                         # Airflow DAGs
â”‚       â”œâ”€â”€ ğŸ“„ bulk_crawl.py             # Bulk crawling DAG
â”‚       â”œâ”€â”€ ğŸ“„ latest_putusan.py         # Latest decisions DAG
â”‚       â””â”€â”€ ğŸ“„ populate_tree.py          # Tree population DAG
â”œâ”€â”€ ğŸ“ api/                              # API endpoints
â”‚   â”œâ”€â”€ ğŸ“„ crawl-scrape-all.py           # Scrape all data API
â”‚   â”œâ”€â”€ ğŸ“„ generate-tree.py              # Generate tree API
â”‚   â””â”€â”€ ğŸ“„ get-latest-putusan.py         # Get latest decisions API
â”œâ”€â”€ ğŸ“ crawler/                          # Scrapy crawler application
â”‚   â”œâ”€â”€ ğŸ“ demo/                         # Main Scrapy project
â”‚   â”‚   â”œâ”€â”€ ğŸ“ spiders/                  # Spider definitions
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ crawl_populate.py     # Population spider
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ recent.py             # Recent decisions spider
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ scrape_list_putusan.py # List scraping spider
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ scrape_page.py        # Page scraping spider
â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils/                    # Utility functions
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ etl/                  # ETL utilities
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ db.py             # Database operations
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ convertDate.py        # Date conversion utilities
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ hash.py               # Hashing utilities
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ items.py                  # Scrapy item definitions
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ middlewares.py            # Scrapy middlewares
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ pipelines.py              # Scrapy pipelines
â”‚   â”‚   â””â”€â”€ ğŸ“„ settings.py               # Scrapy settings
â”‚   â”œâ”€â”€ ğŸ“ plugins/                      # Scrapy plugins
â”‚   â”œâ”€â”€ ğŸ“ venv/                         # Virtual environment
â”‚   â”œâ”€â”€ ğŸ“„ scrapy.cfg                    # Scrapy configuration
â”‚   â””â”€â”€ ğŸ“„ uuid                          # UUID file
â”œâ”€â”€ ğŸ“ extract/                          # Data extraction utilities
â”‚   â”œâ”€â”€ ğŸ“„ config.py                     # Extraction configuration
â”‚   â”œâ”€â”€ ğŸ“„ extractor.py                  # Main extractor logic
â”‚   â”œâ”€â”€ ğŸ“„ main.py                       # Extraction entry point
â”‚   â””â”€â”€ ğŸ“„ url_process.py                # URL processing utilities
â”œâ”€â”€ ğŸ“ img/                              # Project images
â”‚   â””â”€â”€ ğŸ“„ architecture.png              # System architecture diagram
â”œâ”€â”€ ğŸ“„ -docker-compose.yaml              # Alternative docker-compose
â”œâ”€â”€ ğŸ“„ -Dockerfile                       # Alternative Dockerfile
â”œâ”€â”€ ğŸ“„ .gitignore                        # Git ignore rules
â”œâ”€â”€ ğŸ“„ docker-compose.yaml               # Docker Compose configuration
â”œâ”€â”€ ğŸ“„ Dockerfile                        # Docker configuration
â”œâ”€â”€ ğŸ“„ entrypoint.sh                     # Docker entrypoint script
â”œâ”€â”€ ğŸ“„ requirements.txt                  # Python dependencies
â””â”€â”€ ğŸ“„ README.md                         # Project documentation
```

## Prerequisites
- Python 3.7+
- [Scrapy](https://scrapy.org/)
- [ClickHouse](https://clickhouse.com/)
- [S3 MinIO](https://min.io/)
- [Apache Airflow](https://airflow.apache.org/)

## How to Run

```bash
docker run -d --name airflow \
  --network airflow-net \
  -p 8080:8080 \
  -v ".:/opt/airflow/project" \
  -v "./airflow/dags:/opt/airflow/project/dags" \
  -v "./airflow/plugins:/opt/airflow/project/plugins" \
  -e AIRFLOW_HOME=/opt/airflow/project \
  -e AIRFLOW__CORE__EXECUTOR=SequentialExecutor \
  apache/airflow:3.0.3-python3.10 \
  bash -c "cd /opt/airflow/project && pip install -r requirements.txt && airflow standalone"

docker run -d \
  --network airflow-net \
  --name clickhouse \
  --ulimit nofile=262144:262144 \
  -e CLICKHOUSE_USER=default \
  -e CLICKHOUSE_PASSWORD=default \
  -p 8123:8123 \
  -p 9000:9000 \
  clickhouse/clickhouse-server

docker run -d --name minio \
  -p 9090:9090 \
  -p 9091:9091 \
  -e "MINIO_ROOT_USER=minioadmin" \
  -e "MINIO_ROOT_PASSWORD=minioadmin" \
  -v "./minio-data:/data" \
  quay.io/minio/minio server /data --console-address ":9091"
```